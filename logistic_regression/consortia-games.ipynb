{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification Income Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042593,
     "end_time": "2021-09-29T11:38:39.414378",
     "exception": false,
     "start_time": "2021-09-29T11:38:39.371785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Columns | Definition\n",
    "---|---------\n",
    "`age` | Age of Persons\n",
    "`workclass` | Describe work type \n",
    "`fnlwgt` | Financial Weight\n",
    "`education` | Person's education level\n",
    "`martial status` | Person's martial status\n",
    "`occupation` | Person's usual or principal work or business\n",
    "`sex` | Gender of Person\n",
    "`race` | Person's race\n",
    "`capital gain` | Person's capital gain\n",
    "`capital loss` | Person's capital loss\n",
    "`hours per hour` | Earn per hour\n",
    "`native country` | Persons native country\n",
    "`income` | Whether <50k or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"archive/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'fnlwgt', 'education', 'educational-num',\n",
       "       'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
       "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
       "       'income_>50K'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [cols.replace(' ', '') for cols in data.columns]\n",
    "data[\"education\"] = [cols.replace(' ', '') for cols in data[\"education\"]]\n",
    "data[\"marital-status\"] = [cols.replace(' ', '') for cols in data[\"marital-status\"]]\n",
    "data[\"relationship\"] = [cols.replace(' ', '') for cols in data[\"relationship\"]]\n",
    "data[\"race\"] = [cols.replace(' ', '') for cols in data[\"race\"]]\n",
    "data[\"gender\"] = [cols.replace(' ', '') for cols in data[\"gender\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.926519098209614"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# already checked it's safe to remove\n",
    "data = data.replace('?', np.nan)\n",
    "original = len(data)\n",
    "data.dropna(inplace=True,axis=0)\n",
    "without_missing = len(data)\n",
    "without_missing / original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummies for categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['workclass','education','marital-status', 'occupation', 'relationship', 'race', 'gender','native-country']\n",
    "df_dumy = pd.get_dummies(data, columns = cat_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, data is ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consortia testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume a number of players, even split for now, and each comes up with their own benchmark as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_players = 4\n",
    "benchmark_split = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate raw data for each player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_even(data, num_splits):\n",
    "    di_size = round(len(data) / num_splits)\n",
    "    dis = []\n",
    "    index = 0\n",
    "    while index < len(df_dumy):\n",
    "        di = df_dumy[index:index + di_size]\n",
    "        index += di_size\n",
    "        dis.append(di)\n",
    "    return dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = split_even(df_dumy, num_players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we create the benchmark (test data) for each player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_xy = []\n",
    "for di in dis:\n",
    "    X = di.drop(\"income_>50K\",axis=1)\n",
    "    y = di[\"income_>50K\"]\n",
    "    dis_xy.append((X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_bis = []\n",
    "for X, y in dis_xy:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=benchmark_split, random_state=101)\n",
    "    dis_bis.append(((X_train, y_train), (X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train process for each player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def platform_preprocess(X_train, X_test):\n",
    "    # preprocess data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X_train = scaler.fit_transform(X_train)\n",
    "    scaled_X_test = scaler.transform(X_test)\n",
    "    return scaled_X_train, scaled_X_test\n",
    "\n",
    "def platform_train_process(X_train, y_train):    \n",
    "    # model selection and training\n",
    "    parameters_for_testing = {\n",
    "    \"n_estimators\"    : [50,100,150,200,250] ,\n",
    "     \"max_features\"        : [1,2,3,4,5],\n",
    "    }\n",
    "    model = RandomForestClassifier()\n",
    "    kfold = KFold(n_splits=10, random_state=None)  # None, changed from 42\n",
    "    grid_cv = GridSearchCV(estimator=model, param_grid=parameters_for_testing, scoring='accuracy', cv=kfold)\n",
    "    result = grid_cv.fit(X_train, y_train)\n",
    "    print(\"Best: {} using {}\".format(result.best_score_, result.best_params_))\n",
    "    \n",
    "    # model training\n",
    "    tuned_model = RandomForestClassifier(n_estimators=result.best_params_['n_estimators'],\n",
    "                                         max_features=result.best_params_['max_features'])\n",
    "    tuned_model.fit(X_train, y_train)\n",
    "    \n",
    "    return tuned_model\n",
    "\n",
    "def platform_test_model(model, X_test, y_test):\n",
    "    # prediction on test data (benchmark)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8386998614239524 using {'max_features': 5, 'n_estimators': 100}\n",
      "Best: 0.8407742162834 using {'max_features': 5, 'n_estimators': 100}\n",
      "Best: 0.8498314229521938 using {'max_features': 5, 'n_estimators': 200}\n",
      "Best: 0.8486136442643326 using {'max_features': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for ((X_train, y_train), (X_test, y_test)) in dis_bis:\n",
    "    # preprocess\n",
    "    pp_X_train, pp_X_test = platform_preprocess(X_train, X_test)\n",
    "    # train\n",
    "    model = platform_train_process(pp_X_train, y_train)\n",
    "    models.append((model, pp_X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.831207065750736\n",
      "Acc: 0.873405299313052\n",
      "Acc: 0.844946025515211\n",
      "Acc: 0.845927379784102\n"
     ]
    }
   ],
   "source": [
    "# Model quality on individual bi\n",
    "for model, pp_X_test, y_test in models:\n",
    "    accuracy = platform_test_model(model, pp_X_test, y_test)\n",
    "    print(\"Acc: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model quality of each player's model on the consortia benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge bi into bI\n",
    "pp_Xs_test = [pp_X_test for _, pp_X_test, _ in models]\n",
    "ys_test = [y_test for _, _, y_test in models]\n",
    "pp_XI_test = np.concatenate(pp_Xs_test, axis=0)\n",
    "yI_test = np.concatenate(ys_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.8400392541707556\n",
      "Acc: 0.8456820412168793\n",
      "Acc: 0.8461727183513248\n",
      "Acc: 0.8437193326790972\n"
     ]
    }
   ],
   "source": [
    "# Model quality on bI\n",
    "for model, _, _ in models:\n",
    "    accuracy = platform_test_model(model, pp_XI_test, yI_test)\n",
    "    print(\"Acc: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platform combining data and training the big model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Note this is using data transformed individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_train = [X_train for ((X_train, _), (_, _)) in dis_bis]\n",
    "Xs_test = [X_test for ((_, _), (X_test, _)) in dis_bis]\n",
    "ys_train = [y_train for ((_, y_train), (_, _)) in dis_bis]\n",
    "ys_test = [y_test for ((_, _), (_, y_test)) in dis_bis]\n",
    "XI_train = np.concatenate(Xs_train, axis=0)\n",
    "XI_test = np.concatenate(Xs_test, axis=0)\n",
    "yI_train = np.concatenate(ys_train, axis=0)\n",
    "yI_test = np.concatenate(ys_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.847480427422374 using {'max_features': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "pp_XI_train, pp_XI_test = platform_preprocess(XI_train, XI_test)\n",
    "big_model = platform_train_process(pp_XI_train, yI_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.8483807654563298\n"
     ]
    }
   ],
   "source": [
    "accuracy = platform_test_model(big_model, pp_XI_test, yI_test)\n",
    "print(\"Acc: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
